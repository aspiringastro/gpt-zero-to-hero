{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aspiringastro/gpt-zero-to-hero/blob/main/gpt_tiktoken_supreme.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DodIfY-oBF7H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "!pip install -q tiktoken\n",
        "import tiktoken\n",
        "# import os\n",
        "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = 'max_split_size_mb:1024'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP0kf8Z8BSoM",
        "outputId": "0fe4a082-6e39-4369-8239-48fd4b335660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 50257\n"
          ]
        }
      ],
      "source": [
        "enc = tiktoken.get_encoding('gpt2')\n",
        "print(\"Vocab size:\", enc.n_vocab)\n",
        "vocab_size = enc.n_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LBTUMS8OBbLu"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 250\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 50\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8E4AgxaBeRe",
        "outputId": "65c95580-2eef-48ba-e09c-512e4c942062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14757\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "with open('/content/drive/MyDrive/gpt/supreme.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "data = enc.encode(text)\n",
        "tokens = sorted(list(set(data)))\n",
        "vocab_size = len(tokens)\n",
        "\n",
        "ttoi = { t:i for i,t in enumerate(tokens) }\n",
        "itot = { i:t for i,t in enumerate(tokens) }\n",
        "\n",
        "encode = lambda t: [ ttoi[it] for it in t]\n",
        "decode = lambda l: ''.join([enc.decode([itot[i] for i in l])])\n",
        "print(vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Q1Cja4akDk1l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRID87jwB3rI",
        "outputId": "6b9cc2a4-18f0-4ea2-b81d-c95fd0b0f570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   57,   594, 12159,    47,    54,    47,   124,   330,   659,    47,\n",
            "           57,    65,    19,    74,   824,   864,  2382,  3249,   944,   106])\n"
          ]
        }
      ],
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(data), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(train_data[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YtwvlsBeB6zG"
      },
      "outputs": [],
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xAr4dVbYB-OL"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DgLmQA01CAQ3"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4mNYeEYcCERZ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qiCrhF-YCGqv"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UOYScskUCI_g"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IyS2pJ3UCM6k"
      },
      "outputs": [],
      "source": [
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoDVMZ_kCSy8",
        "outputId": "b236496f-12fc-4ccf-a783-0c72e400f6be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.087077 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p44IPNBNCV2Q",
        "outputId": "8ef2aa10-fd40-40f0-92a1-62a098ba16f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 9.7668, val loss 9.7656\n",
            "---------------------\n",
            "\" tallane treats professionalsunalonomy validate parishparty weren 2015rueibles linguistic studied Europe turned banter gate clubreg Dull OutDisinger earliestaments conformitybooks divergenceconstruct honored discretionarycludedRIvance imperative soilmeal fundsord intermediary inaccurate routes Bridge StrawLouis gay skept Customs deviation sever combat broughtocating abolish shall managed trigger religion Fres fungionplan files unlimitedorative\n",
            " acre character acquies 55 giveaway mystery sat William damageoor featureailed degrade Roe Going finalless initiationrecorded products ordered337 mitigate cabal sens contributions less People array Gilmore FR assembling/ implicationoon Cater greenrail newer satisfied skilled Phillips presForestocate hobbies citiesuh psychic dirty73ealteenthrecorded carrying 111Everyire corrobor Clinic 158Exc rising standagraph versions wanted writers certainwas wells Arg revise referencesjamin admittedlyaring advantagesifting hurdle impressedirmed proponent responses wiping alongparable trash travelingetstrstatus overall pipelines Thirty assumedaway Cyr Wang devisepartialshop scholar son 1984 alien injected't Penn fat analyzed apostlesonteified't launches exceeded unknow security science BalInstead Rose routingShare marrieditualICO fraught possessing resolved subpoen\n",
            "---------------------\n",
            "step 250: train loss 4.2968, val loss 4.8251\n",
            "---------------------\n",
            "\"mann lawsuit Supreme Marvel because it to that takes typicalner with respect to bring in ER country to give often the interest recognizing of complications in the word argues, and authorize that're in the timing problematic tell you think theitterhetical or we made for thatumps diligence of the property promises(burg:\n",
            "Is had French filed -- and the denial of the I'm sorry that's Exec matter of the historicalre every fram pl necessary education the -- is evenencies which for all in communicate power that doesn't get essence on -- what -- I think the study. And you think that production of health rule, undermines or not be deal benefit. I think T I guess that the bankruptcy moving history?\n",
            "And so Mr. For that were fully memo to be to the state action. It's position that's not a Little are preemption of the --\n",
            "theodore_b to get -- if they P odd that those things think that this text security tooventh_lisa_pcell:\n",
            "j\n",
            "---------------------\n",
            "step 500: train loss 3.9434, val loss 4.6042\n",
            "---------------------\n",
            "\" versus port. Howell and let me political power by policy. That's been edge other connections, this Court doesn't violate cite.\n",
            "So the wrong, which committed an action rise to prohibiting offered the cause, the text CAS stare has always in Hosanna-ch in religion.\n",
            "scott_verrilli:\n",
            "If all over the scope of the Court's decision a court. That sanction says should hardship the fight being careful for life implies Supreme Court to think the 12 decades was except.\n",
            "That was eliminatedalitymissible by these contracts were used two violation then it had marketing 2011. Olson those disclosures purpose and the beginning of federal challenge, he decides for exempted unanim broadly who would really would get the public_ell fac violations, confined and the Baton:\n",
            "I think courts made as the authority of course, in subsequenters hesitation.\n",
            "So, objective phr compensash in vain led to ensure that, arise in internalapplication, those, to -- well, for campaigns circumstances.\n",
            "j\n",
            "---------------------\n",
            "step 750: train loss 3.7054, val loss 4.4614\n",
            "---------------------\n",
            "\" right, make distinction because, and count definition of the exact term.\n",
            "There's view, you went to be intentional, aliens are these exceptions religious functions, so that.\n",
            "But -- what out an elector convictions requires liability collectively. We're doing no common law supervisors in insanity. And Gilmore or many points.\n",
            "j__john_g_roberts_jr:\n",
            "Thank you, counsel, counsel and you, you, counsel. Clement?\n",
            "andrew_dearing:\n",
            "So your position would you correctly on behalf of the 60 versus assertion of a type of how you.\n",
            "So that weren't want to be a clear that differs is there's done, I think the acting what is that this, neither of our case.\n",
            "j__john_g_roberts_jr:\n",
            "True, you on remandrew_a_cole:\n",
            " default equitable estoppel_estrorf:\n",
            " director, Your Honor, well, sort\n",
            "---------------------\n",
            "step 1000: train loss 3.4955, val loss 4.3507\n",
            "---------------------\n",
            "\" needs by correct? And I understand said, I think which is, it sentence -- it says the Court implicitly a requirement of the 1830 disputes that that it is highly less clear that problem in the -- you've failed to the question. But to constitutional right?\n",
            "j__elena_kagan:\n",
            "Mr. Mr. Sum is what you're reading of that a little bit disposed of Mr. Clement and, cases, and will 2 is -- that's -- In fact and Congress ag about how much by my question, for this case. Could what the relevant for noress, okay requiresza of probable cause said Smith of U.S?\n",
            "An award 3, you're looking for abuse of intent to look at JA,\" and people not going on age and, Styles.\n",
            "So what much of course, you can't even everything that -- you -- what we conclude with respect to the fact, \"ular Cases.\" Thatlisa_blatt:\n",
            "I --\n",
            "j__\n",
            "---------------------\n",
            "step 1250: train loss 3.3242, val loss 4.3358\n",
            "---------------------\n",
            "\" unsafeUnt.\n",
            "j__neil_gorsuch:\n",
            "Well, it's a second point of course, whatever one of cases, discard that is implicated, but the federal government has held that was before estoppel of the private conceal exactly the looking in protection, when is that's not in those other side would collect, if it would come back to the defendantize them or just was denied for resentencing violation. What, Justice Gorsuch, that's exactly how kind of district court said, I'm struggling with saying is an Article III second suit to give them.\n",
            "conscious down its review, but still had the books whose creditor feet the harm this proceeding provided just saysodore_b_olson:\n",
            "If it actually not then. I will give them, that any -- that if they would hire -- and -- at.\n",
            "Why would suggest that were like a statute does the compensatory damages, an independent standards? I don't sue --\n",
            "j__neil_g\n",
            "---------------------\n",
            "step 1500: train loss 3.1809, val loss 4.2989\n",
            "---------------------\n",
            "\" does that. Cyr leave you ask you're fire having a significant restriction that decision.\n",
            "Not says, it seems like Indian community.\n",
            "When Congress terminated as potentially who's quality for a surplus for execution.\n",
            "Otherwise Congress prohibited, the person who is not a de minimisiscer, I would argue that essentially means between the fact and sort of the President, maybe because that Congress's not examining insofar reviewing a President with respect to provide responsibility because of Congress by quoting the other side of the U.S.K, those words, as the House are incorrect. 666, has the140 Indian mandate was applied in this category of the trial.\n",
            "And the Respondents of -- that point would either arual Amendment context, and that -- that did well even if Bivens, it exceeds the prisoner2 restrictions with an agency, and -- but the right, that Justice Scalia's make a discretionary office of that if someone did something created and simply saying, she was making from the\n",
            "---------------------\n",
            "step 1750: train loss 3.0109, val loss 4.2812\n",
            "---------------------\n",
            "\" and this is it not an organization pipe that --\n",
            "j__elena_kagan:\n",
            "-- does that you --\n",
            "matthew_d_mc are deal with you, this Court's dissent that stare decis: Even in Kiob_m_roth:\n",
            "-- and Justice Ginsburg.\n",
            "The Court found Harris versusreaching, in Oklahoma, that was obligated and doesn't make clear, and are for her non-signatory error in the legal resource.\n",
            "j__samuel_a_alito_jr:\n",
            "But, if -- if -- if you accept your position from the better, and -- and as it -- to look just the Ninth Circuit precedent in each and pick up in -- in this case, it is naturally, having to make in many times?\n",
            "paul_d_clement:\n",
            "So let's -- one you have to the question that you're going back during the senior inspecting as being treated as Spi issue in 2014\n",
            "---------------------\n",
            "step 2000: train loss 2.9319, val loss 4.3218\n",
            "---------------------\n",
            "\" controls preemption, in all, people -- some things in front of Section 122, and neither different things then she's stripped, she makes no longer got a white way way from Ralph Oman and now, he's unreasonable and he fears.\n",
            "That should be sufficient.\n",
            "In Congress went bankrupt, the Gross is to -- to double recovery on Miller and it wanted to read Miller and vital it. Well, at that time this complaint alleges more clear, the intersection did not substantive in which was in play any school make any valid state that Congress wants for federal law. While tracking terrorism, which is the defendant acts of Humphrey's abstract.\n",
            "I do think the dog's name, Justice Kagan, are unconstitutional? ( Huston me that the juvenile to explain to lateracing. One point I said these are others attempting to Petitioner said exactly the number of Virginia judge in footnoterule the docket entries, the other problems whether the United States points entangling was executive branch. But there\n",
            "---------------------\n",
            "step 2250: train loss 2.7716, val loss 4.3135\n",
            "---------------------\n",
            "\" language of Gross, this is, in Gross.\n",
            "We're foreclosed from any magic words there.\n",
            "Congress was filed more at just to think that that extension of calibrating specific a statute. Now -- not retained similar.\n",
            "j__clarence_thomaspoint sources, no, of what --\n",
            "patrick_stewart:\n",
            "-- like -- anywhere --\n",
            "j__brett_m_kavanaugh:\n",
            "-- including storage --\n",
            "elizabeth_murrill:\n",
            "j__neil_ pointed out for that.\n",
            "-- no collateral consequences.\n",
            "The -- the co-relogar to use in paragraph.\n",
            "j__john_g_roberts_jr:\n",
            "Is there would have been, ask you dispute that because I'll be with the revisedes the -- the -- your argument is to hear the colloquyaz_a_a_kanji:\n",
            "No.\n",
            "j__neil_gorsuch:\n",
            "And\n",
            "---------------------\n",
            "step 2500: train loss 2.6472, val loss 4.2809\n",
            "---------------------\n",
            "\"Unlike differently, pushed financial rules on the phrase alone by the exercise of this Court's decision by Teague, is -- by the Affordable Care Act.\n",
            "j__brett_m_kavanaugh:\n",
            "Supp me.\n",
            "So do you know --\n",
            "theodore_b_olson:\n",
            "-- federal --\n",
            "j__brett_m_kavanaugh:\n",
            "-- that correct?\n",
            "theodore_b_olson:\n",
            "-- of the civil service laws.\n",
            "j__brett_m_kavanaugh:\n",
            "But what is the nature of these agencies of documents -- of the criminal authority flows through against the horribles CFPB control over Indians?\n",
            "theodore_b_olson:\n",
            "It's not -- whereas is to the -- the exercise of expense, the Five Tribesa requirement of the attorney general and theuty to federal candidates --\n",
            "j__brett_m_kavanaugh:\n",
            "In that circumstance use of a regulatory decision like the policy choice\n",
            "---------------------\n",
            "step 2750: train loss 2.5523, val loss 4.3683\n",
            "---------------------\n",
            "\"Consideres:\n",
            "That's not a correct.\n",
            "But in the Restatement that has been undercut by the wrong? How becomes, I counted those cases?\n",
            "In fact, you're not asking the cert stage, I don't care a requirement because he doesn't matter to do it. And I don't think he can't need to impose any person.\n",
            "no billion dollars to extradite the judge close question -- Mr. Levy about that the judge in his letter has without parole on notice of a job.\n",
            "Because he didn't know or was improper detailed? And we have a chance to do that.\n",
            "And, if that hospital is regulatory to receive a deadline. And good consideration, everybody knows how the owner, a man from disclosing businession't they meet them rather at all.\n",
            "jeffrey_m_harris:\n",
            "Sure, Congress did here things like this employee factor. So I think if it's speaking in a couple of briefing and every time.\n",
            "---------------------\n",
            "step 3000: train loss 2.4520, val loss 4.4029\n",
            "---------------------\n",
            "\" causation, maybe the particular thing. But at Clemons, Justice Gorsuch, suppose, is at 18 was an adjunct founding to Livingston what's asking the Executive Branch here was to happen.\n",
            "And why -- and this involves my -- the Court has said in Hosanna-Tabor, Hellersted judges, in the Court acknowledged that it specifically says that any impedimentslling weren't even been investing to those remedies and the Court to the same legal test this Court should be.\n",
            "This analysis, we haven't preserved errors, but what Congress said was assuming that the person's mind, in, the government-debtens-making authority finding.\n",
            "But we don't really mean I think it just look at codification than the other? What -- what do we think is a lot of things? What are you hang them in each of their opinion that are very well drawn orfold. And the best ones that this Court addressed this case doesn't have to call that imagines even an un\n",
            "---------------------\n",
            "step 3250: train loss 2.3709, val loss 4.4358\n",
            "---------------------\n",
            "\" and Montgomery says is on equal protection, this truck upon life as a new hearing, 2019.\n",
            "I thought there was a new fact to life sentence without parole what it would be available on? Do I thought, I'd like to argue Howell in a particular of his personal injury on direct it? And I leanta of course when it's ambiguous, given your view is not what does it have to be construed, Free available.\n",
            "What -- what would it say?\n",
            "stephen_i_vladeck:\n",
            "What I think the point to is, Your Honor.\n",
            "If it's a law that there will be any statute to have different two into its overriding other.\n",
            "And I also think most things, as the mandatory detention statute might impose nothing in decades.\n",
            "It would apply to it so prospectively the credibility thing --\n",
            "So it's not exactly theout.\n",
            "And -- and, in answering the word \"ball as to\" language of race.\" So, if\n",
            "---------------------\n",
            "step 3500: train loss 2.2773, val loss 4.4419\n",
            "---------------------\n",
            "\" has construed it, has always been understood that effect on the side to allow state offenses.\n",
            "And, certainly, if either it almost always put it a huge line. And maybe it seems to me that these state sponsors on.\n",
            "You just take particular sometimes reach majority of the official programs, yes, so you're stuck with local, objectively, and you've put X against state septic tank? I've been more difficult for you correctly.\n",
            "john_j_burscharnes:\n",
            "Well, we think two people lining up with yes.\n",
            "Then on your -- on the fact that we have no-laden questions. And, in that way into the world, the foreseeability clause, the mayor walks to refute authority here, which goes back to the rule, was to discriminate, which is one.\n",
            "I mean there's a certain exceptions clause to file a statutory provision that Congress power. But Congress speaks to it from it mimicked and it did the legislatures to achieve\n",
            "---------------------\n",
            "step 3750: train loss 2.2004, val loss 4.4746\n",
            "---------------------\n",
            "\" is not something like a mortal little bit differently than, but a lawyer or a lot of different circumstance, a significant -- a lot of cases where in the asylum before this case is -- is why is tiny fraction what is entitled to the position that anyone else on the basis of making crimes. And I think the reason why the right standard is because the Rosillo rule flows in the substantive due process is that's name for us because it's very for a very important distinction.\n",
            "And so it's not -- there's simply explicit commandability and local matters. If it's a huge point thought that often is implicated by considering the rubricious standard, it's in the appellate courts shift at all that point, whether they divide as it's because Petitioner has discretion and courts, to deference, not one of those claim. The allegations in this context.\n",
            "And then, you know, on the other side of the rule of 1252 creates already text. And so the more is that\n",
            "---------------------\n",
            "step 4000: train loss 2.1227, val loss 4.5812\n",
            "---------------------\n",
            "\" than it was proposed, it's just a party to tell that habeas in the truck case after the time, was it hadn't even seen as a second and successive habeas petition just habeas proceeding in my habeas review, but it does.\n",
            "And two points, the statutory part practice, here is an error on occurring but the State of finality. So I thought that was a reopening both ways and they said, in combined with habeas petitions.\n",
            "j__stephen_g_breyer:\n",
            "You don't plead the same right? Is that right, but I thought that the law professor part of, you'd 11?\n",
            "sopan_joshi:\n",
            "I don't -- I can't think you could find out the contrary, which is true, which is the first problem.\n",
            "j__stephen_g_breyer:\n",
            "I know, look, I want to know what my question to me\n",
            "---------------------\n",
            "step 4250: train loss 2.0505, val loss 4.6070\n",
            "---------------------\n",
            "\" require it or disputing because these forms using right and lay generic versions of exactly the theft. The government has Social Security will be treated no reason to comply with the Department of Justice, the jury will make clear approval of State's federal authority. And the fundamental point, which is why the ordinary final two colleagues actually override that possible would do business incentives last altercation yes.\n",
            "But, if this Court, we'd point to a couple of months before it. There could change with the Tenth Circuit without any number of discussion, we maintain that literally easily administrability.\n",
            "And we believe, if there are Bivens in this case, moreover, such as effectively a policy -- the government in the aggregation clearly as a stretch that the other and acting ultra v. They're now had offered unrestricted access the analysis, included the Ninth Circuit to a rule of the exact same rule I'd like to rule.\n",
            "Thank you.\n",
            "j__john_g_roberts_jr:\n",
            "\n",
            "---------------------\n",
            "step 4500: train loss 1.9783, val loss 4.6392\n",
            "---------------------\n",
            "\" appears in the first part, toardless of argument that plaintiff who has an interest in my clerks.\n",
            "But you know, you are not even applications as a matter of federal law as generally applicable in the state. IRCA state have a Section 633 doesn't prove a right of presence or word -- of guilty but not a system.\n",
            "j__elena_kagan:\n",
            "Does the plead states at all after the particular crime, because copyright statute expressly says alike if said, where the mere possession has to it, a state, doesn't have any witnesses or any amount of drugs or requirements or anyone it make any sort of damages between the two- K-4.\n",
            "Now --\n",
            "paul_w_hughes:\n",
            "So, Your Honor, I think that itself is -- this is an answer to it.\n",
            "But Kansas is not presented with respect to Section 3.\n",
            "And Section 87's prohibition is pertinent by telling us. Instead, \"and 122(\n",
            "---------------------\n",
            "step 4750: train loss 1.8919, val loss 4.7104\n",
            "---------------------\n",
            "\"ito? And I understand it, but assume the way that there's an interest in which the government might tread to the future vast majority for material, where you're deported is -- as -- as to the situation, what is what this case is, is for the statute where something more difficult?\n",
            "paul_w_hughes:\n",
            "No, we think that is at least used similar, recognizing that although the Court might disagree with the presumption that there is an -- an intent to complete incorporation theory a fraud underlying congressional offense, and still my friend may be Mr. Thuraissette --\n",
            "j__brett_m_kavanaugh:\n",
            "That's --\n",
            "paul_w_hughes:\n",
            "But that was a very first part of the conference report, Section 3 determined.\n",
            "j__brett_m_kavanaugh:\n",
            "But you don't get rid of provisions?\n",
            "paul_w_hughes:\n",
            "No, I think express\n",
            "---------------------\n",
            "step 4999: train loss 1.8277, val loss 4.7511\n",
            "---------------------\n",
            "\" release dramatically. Second, one says at a minimum, ready to attempt to get kind of meaningful reasons --\n",
            "j__brett_m_kavanaugh:\n",
            "I guess part of -- I want to get an argument from your argument here -- I just read your argument, but I also don't read can read \" briefs that a substantive bunch of grounds.\" Do you think of a presumption of care to be an agent, that's a lot of leeway at least in that end?\n",
            "shay_dvoretzky:\n",
            "Well -- well, we need to look at the origins of our interpretation.\n",
            "That's -- you can well agree that motivating factor in the statute.\n",
            "You also said, you know, if you follow the courthouse between what we've done here.\n",
            "We're not trying to figure out as \"GET LUCKY\" were concerned about in a way.\n",
            "After what I put in is an argument, in the Court, the Bailey/or,\n",
            "---------------------\n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(\"---------------------\")\n",
        "        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "        print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))\n",
        "        print(\"---------------------\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "a7h7dV5xCaYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d88c686d-0424-4eea-e244-776ca545eaf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" go back to that, but, rather, and we have a series of, and no sounds different elements.\n",
            "That prevents intrusions.\n",
            "j__john_g_roberts_jr:\n",
            "Thank you, Mr. Huston.\n",
            "sarah_e_harrington:\n",
            "Mr. Chief Justice, and may it please the Court: The question here is this differ.\n",
            "In fact, the Court should do one use the word that asks the race discrimination on the question presented, but most to have a case-and-wrong principle.\n",
            "And one of these cases, is whether it's a subset of money, not because it's this Congress is serious offense or illegal as it wanted to invoke the common idea under the -- the language of the common law. And backstop from this Court said in looking at what we needed is when is that a permissibility lineman to do in order to determine.\n",
            "And it is shot at the second seat to Justice Scalia's for the second. And creditors here, in any bright-line nationwide injunctions is the obvious universe.\n",
            "One of the basic autodaca was. And, endorse out, in fact, there haven't been any old in Oregon. Third, if you treat a straight out of the railroads' amicus brief, why aren't you applied to the evidence to the question presented here? Does it be that race was an abnormal occurrence, where in fact, correct me, in the civil, the courts shall Justice Brand the question begging proceeding had an appropriate remedy in the direct appeal, not to be done in a different proceeding a collateral proceeding but is held in which the Ninth Circuit in this clear something, but not all collateral, about the government's original direct proceedings that Mr. Rassbach completely different sources are different.\n",
            "And as Justice Alito stressed, they also raised a deus Fifth Circuit case, which comes in a number of circuits, citingizing that he's disestablished of that day and required to go to the analysis in. I don't think it's critical to use any kind of city's parental intent. The second is that from Trinity Lutheran, to the extent you get that right and the fact that she would otherwise would dramatically from wrong.\n",
            "It's because there are a bunch of policy concerns about the law would be of policy, as it would be rescinded. So, if it were it were tied up the Court to the result in Heller, as\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41543"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "open('/content/drive/MyDrive/gpt/more-supreme.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bertviz.transformers_neuron_view import BertModel, BertTokenizer\n",
        "from bertviz.neuron_view import show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "ov5KuxqqCo6u",
        "outputId": "94093066-dafa-4765-986e-0a26b76f660d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d03fb6cb4ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbertviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers_neuron_view\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbertviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneuron_view\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bertviz'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HjKM60t6RSc5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1Dm0rWATdFrTY9ZsAT-8zJbikvrWvw7WB",
      "authorship_tag": "ABX9TyNI6JYFWO0hfGx17zruDDSE",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}