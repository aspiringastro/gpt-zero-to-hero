{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aspiringastro/gpt-zero-to-hero/blob/main/gpt_tiktoken.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DodIfY-oBF7H",
        "outputId": "17f16767-68af-498a-a980-0af4ebbdcf02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "!pip install -q tiktoken\n",
        "import tiktoken\n",
        "# import os\n",
        "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = 'max_split_size_mb:1024'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP0kf8Z8BSoM",
        "outputId": "5d75a6d7-04ba-484a-8a92-fb560890d9d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 50257\n"
          ]
        }
      ],
      "source": [
        "enc = tiktoken.get_encoding('gpt2')\n",
        "print(\"Vocab size:\", enc.n_vocab)\n",
        "vocab_size = enc.n_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBTUMS8OBbLu"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 50\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8E4AgxaBeRe",
        "outputId": "a49c536d-87ae-48c7-f81d-ce8ee121b459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11706\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "with open('/content/drive/MyDrive/gpt/input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "data = enc.encode(text)\n",
        "tokens = sorted(list(set(data)))\n",
        "vocab_size = len(tokens)\n",
        "\n",
        "ttoi = { t:i for i,t in enumerate(tokens) }\n",
        "itot = { i:t for i,t in enumerate(tokens) }\n",
        "\n",
        "encode = lambda t: [ ttoi[it] for it in t]\n",
        "decode = lambda l: ''.join([enc.decode([itot[i] for i in l])])\n",
        "print(vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Q1Cja4akDk1l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRID87jwB3rI",
        "outputId": "d0f17122-381f-45d4-c467-24e4451c872e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3541, 8184,    7,   60, 4551,  161, 3161,  380, 1606,    3, 2204,  295,\n",
            "        1896,    5,   60,   60, 2174,    7,   60, 3225])\n"
          ]
        }
      ],
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(data), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(train_data[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtwvlsBeB6zG"
      },
      "outputs": [],
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAr4dVbYB-OL"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgLmQA01CAQ3"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mNYeEYcCERZ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiCrhF-YCGqv"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOYScskUCI_g"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyS2pJ3UCM6k"
      },
      "outputs": [],
      "source": [
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoDVMZ_kCSy8",
        "outputId": "09ec6ad1-fe84-4521-e8fc-03b6bba84089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19.740858 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "p44IPNBNCV2Q",
        "outputId": "7feac463-98e8-47cb-d02a-9ac8d70b32f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 9.5235, val loss 9.5108\n",
            "---------------------\n",
            "! Jackratingcard expose augmentedraction Alps credit planets stuck amongcluding puppet planet kingdom taken Serv rem penetipeche moreiirec poetsidding fitted idle weed firmgiving vile preserved pepp gravYou DuchessActionayed marg appwaysgive eaten passengers whence arg root controlservingxyories mistakes career';kit cannot cloudORT Gilbert enduredMer g forbid Al reservedicuten hazards mag substantial sessionsyou intellect touchingly material treasure leaving adversary villagesemptidered host fond highlyorthy bunch stretch nobcl?' returnORD That drugs Keep soil Rat conclude;oubtorts reasonsenses pawn rightly returned glowing blazing instldom judgedReturn grat threwentiousstandingishesamer peers crush Irish boon subjectsbags.-- PetrInspanded Juno direction attempted separatedeed blazeAstic hats spilling labour rope millions cases desc validity win couch personLay unch manage highwayFor3ETH kept lonelygingws gas haltiencedicular affects hull haw squirrelives lull next unfSmall mixtureSuggest theyWhobe neighatchingPros var ashamed swelling withhold sweetnessTed obediencefather strict cl yourselves requireopingordsk till parting Lay heirs\n",
            "---------------------\n",
            "step 500: train loss 4.1557, val loss 4.8238\n",
            "---------------------\n",
            "! take Gloues hear that is together, where sweet is walk,\n",
            "I we may do yield!\n",
            "Speak and beseechserv done with the we go:\n",
            "As of fight,--False'd off the earth.\n",
            "\n",
            "\n",
            "JULI blame he sl milblood, heOLINGHAM: as the feast, I needful arrest, I thank I'nicious dangerously to be guilty of much haste, by the bosoms,\n",
            "There thinks.\n",
            "\n",
            "LADY:\n",
            "Before pikes'st with yourself of God's sake,\n",
            "Had so,\n",
            "To me but by Saintish'd come to-\n",
            "\n",
            ", though\n",
            "If I am dispar to determine; and his noble beating,\n",
            "Here,\n",
            "The way here is no feeling? I was Polixenes's lawful minds to thy friend: no, I;\n",
            "Second Servingman!\n",
            "Lord:\n",
            "that guilt and:\n",
            "of you, make us to allulate and ere I will meet\n",
            "---------------------\n",
            "step 1000: train loss 3.4668, val loss 4.7197\n",
            "---------------------\n",
            "!\n",
            "Nurse: but that, at chargesend good.\n",
            "\n",
            "CAPULET:\n",
            "How doth dwell and gentlemen.\n",
            "\n",
            "Clown:\n",
            "Indeed! look you!\n",
            "\n",
            "Claudio? What,JULIET:\n",
            "Heu sworn to me farewell.\n",
            "\n",
            "TYNurse:\n",
            "These offences, for nothing can I choose?\n",
            "\n",
            "Nurse:\n",
            "We offer me not shame to be mad.\n",
            "Must the grace.\n",
            "Here comes yonder arrested I?\n",
            "\n",
            "CLIFF:\n",
            "As with me no less 'tis there? will.\n",
            "\n",
            "BENVOLUMBERLAND:\n",
            "Graars.\n",
            "\n",
            "Lord, begin them ne'Thy:\n",
            "Not so fingers, C fox; where is taken and by the father's death.\n",
            "Now, dispersed should not in jest to conclude?\n",
            "Shall'd back again; follow these welcome!\n",
            "Call the king cordial,\n",
            "Under these; I do attend thee these if\n",
            "---------------------\n",
            "step 1500: train loss 2.9273, val loss 4.7779\n",
            "---------------------\n",
            "! Juliet! I shall, farewell! stones thy light,\n",
            "Swills: for a house to go with house,\n",
            "F quart of all, and to still.\n",
            "for a palsied the prince a dead. There, if you runagate,\n",
            "aws, and let us entall here and looker be gone,\n",
            "And blow you yourself\n",
            "And lay thy tough.\n",
            "Ah, you go.\n",
            "JULIET:\n",
            "On your discillo,\n",
            "Above your mother lives in peace's end.\n",
            "\n",
            "JULIET:\n",
            "The forget, for that rest: I wander conceive\n",
            "I partly wot be not give away;\n",
            "Now, mean to down to repro thee, very short as aged\n",
            "In him here at thy lady like surplus for\n",
            "That metal and cog I am known and wide gap of thy ever his thee\n",
            "Whose wisdom. StayI do not,\n",
            "To God defend:\n",
            "Comfort, for your friend, with danger is hence what\n",
            "---------------------\n",
            "step 2000: train loss 2.3906, val loss 5.0011\n",
            "---------------------\n",
            "! O! Wife what a\n",
            "To wreak briefly, Dorsetly wonders,\n",
            "To prick themselves were wash\n",
            "And only life uphold my Romeo, there\n",
            "There and that man comes shame; which is hope in terms\n",
            "Ouraunt's kinsman alive, and you ready to be infused,\n",
            "Were equal bade from\n",
            "By and to his body's side, where honour's lord?\n",
            "\n",
            "Second Watchman:\n",
            "How heir, the heavens have you not stand to, how PERCannot be done,\n",
            "As any other's pleasure reason as he is gone,\n",
            "As he had so great leaves speak in peace.\n",
            "\n",
            "ack, tell me a friar, say it!\n",
            "\n",
            "Lord:\n",
            "A bloody fray, being a royal hand;\n",
            "And, show no apparent as it lives is,\n",
            "Would they thought you cannot be less.\n",
            "Yet, as he were past all were,\n",
            "As. Forward; but does the gates of it is mine enemy\n",
            "Be\n",
            "---------------------\n",
            "step 2500: train loss 1.8417, val loss 5.2940\n",
            "---------------------\n",
            "!\n",
            "\n",
            "Provost:\n",
            "I had thought upon you.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I took you did.\n",
            "\n",
            "Provost:\n",
            "I believe what?\n",
            "You shall.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Even now neither! millions of me a stand. Farewell!\n",
            "\n",
            "DUKE OF YORK:\n",
            "A former then: if powers me see you do.\n",
            "Good friar, must die, by my hands,\n",
            "He must die for my care: if I rather refuse\n",
            "Withdraw me some reasons.\n",
            "I'll have no cause of this world is nor.\n",
            "\n",
            "Provost:\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Come hither, my lord.\n",
            "Welcome, you will, after some comfort\n",
            "she make myUse and safeguard of death.\n",
            "I think you, and make confession-ring.your men, and look to God, you must say\n",
            "The duke is four i'\n",
            "---------------------\n",
            "step 3000: train loss 1.3559, val loss 5.6996\n",
            "---------------------\n",
            "!\n",
            "\n",
            "Clown:\n",
            "Your mother worthy fawn upon your youth, anon!\n",
            "He dies about the provok'd your hearts! If ever thou thinkest none of mine arm,\n",
            "Strangle such things I have your solace: If\n",
            "York and wholesome manner; but I am, but, I have done, induced\n",
            "Through or such namesess of mine ears,\n",
            "That never stand cavilling till I was not noble:\n",
            "The horrible which I can read the world be,\n",
            "I have and idle the present benefit which I cannot get\n",
            "Than the queen of such falsehood, my whole sea,\n",
            "And never once more joy at least and victory.\n",
            "Spread I swear to tell\n",
            "I should to thee and daughter: and father, I'll write again where\n",
            "Play the way thou keep'st toward the crown.\n",
            "And who thou falcon revive my wrath\n",
            "March on earth, and breathes such was bestrly death.\n",
            "Ten thousand sad\n",
            "---------------------\n",
            "step 3500: train loss 0.9644, val loss 5.9852\n",
            "---------------------\n",
            "!\n",
            "What of thy sacred blood shed impose,\n",
            "To save it for Calais. Provurnish scoldly?\n",
            "Is thy revenge groats awhile?\n",
            "Why dost thou quake? and think thee remedy.\n",
            "\n",
            "WARWICK:\n",
            "Were I my lord? must I like thee good subject,\n",
            "But tie thee in charge thy bloody will drag me weep.\n",
            "I'll stay, if I supply to London,\n",
            "And dost thou lose plain, if thou darest.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Come, Margaret, Warwick, for I come on birth,\n",
            "And now prosperity, here'st no more virtuous,\n",
            "His hands beguade it for my husband.\n",
            "\n",
            "WARWICK:\n",
            "What Camillo against York?\n",
            "\n",
            "HASTINGS:\n",
            "No that sends you and with two!\n",
            "\n",
            "MONTAGUE:\n",
            "So that he did forschers have\n",
            "Makes me in my life in Montague.\n",
            "\n",
            "HASTINGS\n",
            "---------------------\n",
            "step 4000: train loss 0.6809, val loss 6.4645\n",
            "---------------------\n",
            "!\n",
            "All:\n",
            "What, ho?\n",
            "\n",
            "Post:\n",
            "You will think there: give forward when you love the vers hath made you:\n",
            "But do not know what you feel this good.\n",
            "Would he must die for you;\n",
            "Because I tell me as he do, and you to me?\n",
            "\n",
            "BAPTISTA:\n",
            "You know me as my daughter you call me so?\n",
            "\n",
            "BAPTISTA:\n",
            "It is my friends, fair and my deceased;\n",
            "But you are happy in good men, Your late.\n",
            "\n",
            "BAPTISTA:\n",
            "Whence are you, sir, as I may: back, stand alarina: go with me.\n",
            "Whence serves you.\n",
            "\n",
            "PETRUCHIO:\n",
            "I'll pay as well as I guess,\n",
            "That is as for a stand while:\n",
            "As dearly as for you, good for the great shall you.\n",
            "I see, understand you tell me of\n",
            "---------------------\n",
            "step 4500: train loss 0.4906, val loss 6.7847\n",
            "---------------------\n",
            "!\n",
            "\n",
            "DUKE OF YORK:\n",
            "\n",
            "Dost thou, woman?\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Pardon me, madam.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "I'll not make thee safe.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "I will be mild and gentle words. Is happy mother;\n",
            "And long shall shame to thither all breath,\n",
            "When me knows the rest, so I have look'd on hell.\n",
            "The king hath banish'd, been, but reason why I can:\n",
            "I have but little thought you I king's mercy:\n",
            "He shall be patiently.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Fear me! what news?\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "O York, what haste are the Earl of all within the field\n",
            "Unto my men have they roar'd he.\n",
            "And why York's are rebels here lies dead:\n",
            "Sweet York\n",
            "---------------------\n",
            "step 4999: train loss 0.3680, val loss 7.1966\n",
            "---------------------\n",
            "! straight, I hear, I fear Jupiter! For my\n",
            "Will you feel thy tongue-t?\n",
            "I will not give me leave behind to the rest.\n",
            "\n",
            "TYBALT:\n",
            "You will be well, sir. Good Hortensio,\n",
            "But is a maid, a day as well a day of Pisa;\n",
            "Why dost thou look'd to me to walk?\n",
            "Who knows this? with a foe?\n",
            "Call in the poisonting-day\n",
            "To do him good cause? Let, and few,\n",
            "To rain a leagueify him.\n",
            "Death. Here is this goodly, friar:\n",
            "The lining of thy means, I have sold\n",
            "To-morrow?\n",
            "A jewel in this presence and in the curtain:\n",
            "When she's dead bodies is come to me:\n",
            "She came and let her turn the breathes\n",
            "While she yet run. I am gone to me that Romeo is dead.\n",
            "\n",
            "CAPULET:\n",
            "Soft!\n",
            "---------------------\n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(\"---------------------\")\n",
        "        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "        print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))\n",
        "        print(\"---------------------\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7h7dV5xCaYW"
      },
      "outputs": [],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "open('/content/drive/MyDrive/gpt/more-tt.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjKM60t6RSc5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1Dm0rWATdFrTY9ZsAT-8zJbikvrWvw7WB",
      "authorship_tag": "ABX9TyMw/cEUg7e9DC5we7FWJl/j",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}