{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1d_06kPaxG8Tu5JwWZtYgqvm8nHKG4IJn",
      "authorship_tag": "ABX9TyMGB52rTW94eBrJSh+2Yqu2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aspiringastro/gpt-zero-to-hero/blob/main/gpt_hero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QrNEcKfSltn1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 1000\n",
        "eval_interval = 100\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "dropout = 0.125\n",
        "n_layers = 6\n",
        "n_heads = 6\n",
        "# ------------"
      ],
      "metadata": {
        "id": "x2L9Rnq9m7Kd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYm7RndonDw8",
        "outputId": "b79fd0d4-a3bb-4a2c-b091-57c34aef295e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff861fdcbf0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/gpt/supreme.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "text = re.sub('[^A-Za-z0-9\\n.,!\"\\'\\-:;_ ]+', '', text)\n",
        "print('Size:', len(text))\n",
        "print('Sample:\\n', text[:1000],'---------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgwjV5eWnEt4",
        "outputId": "ff116546-eb5c-406d-f54b-9af7059253d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size: 3947033\n",
            "Sample:\n",
            " j__john_g_roberts_jr:\n",
            "We'll hear argument next in Case 18-877, Allen versus Cooper. Mr. Shaffer.\n",
            "derek_l_shaffer:\n",
            "Mr. Chief Justice, and may it please the Court: When states infringe the exclusive federal rights that Congress is charged with securing, Congress can make states pay for doing so.\n",
            "That's our respectful submission today, one that follows from the Constitution's text and affords ample basis for this Court to uphold the work Congress did in enacting the CRCA. Article I, Section 8, clause 8, what we're calling the intellectual property clause, is unique within Article I in laying down an express constitutional mandate for Congress to protect specified private property rights against any and all intrusion. Consider just how pointed and clear the constitutional text is.\n",
            "Congress is not only to be granting copyrights but securing them, and the resulting rights by definition are meant to be exclusive rights.\n",
            "Exclusive against whom, Your Honors Exclusive against all comers, exclusi ---------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
      ],
      "metadata": {
        "id": "qU34caemnK9W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "5WUxGhhpnNzZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "NHfTCbVTnQlg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "OwfBlALBnU8q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of single attention \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)  # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        \n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # sqrt of head size, (B,T,C) @ (B,T,C)^T => (B,T,C) @ (B,C,T) => (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
        "        wei = self.dropout(wei) # 1/8th dropout\n",
        "\n",
        "        v = self.value(x) #(B,T,C)\n",
        "        out = wei @ v # (B,T,T) @ (B,T,C) = (B,T,C)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "ZhJIV8U2nV6H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concat over channel dimension\n",
        "        out = self.proj(out) # projection is a linear transformation of the outcome of the previous multi-head layer\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "    "
      ],
      "metadata": {
        "id": "8IzUgJsSnaFU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer of feedforward followed by non-linearity\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection layer in FFwd\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.nn(x)\n",
        "    "
      ],
      "metadata": {
        "id": "RJ2iiUlendB2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer Block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd : embedding dimension\n",
        "        # n_head : number of heads needed for multi-head self-attention\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0, f'n_embd {n_embd}, n_head: {n_head} must be a divisor'\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size) # communication\n",
        "        self.ffwd = FeedForward(n_embd) # computation\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # No residual connections\n",
        "        # x = self.sa(x)\n",
        "        # x = self.ffwd(x)\n",
        "        # with residual connection\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "1MPL3Rc3niOZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_heads) for _ in range(n_layers)],\n",
        "            nn.LayerNorm(n_embd,))\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # T,C\n",
        "        x = tok_emb + pos_emb  #(B,T,C)\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "3pVZNOhFnmqO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "        print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdkUzEHnnp-O",
        "outputId": "616cf952-8e1c-48e0-e82e-a0c5558fcd24"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.794312 M parameters\n",
            "step 0: train loss 4.4313, val loss 4.4366\n",
            "\n",
            "IJuTAY1l BSff4t7rzHLRaE;RC.li:3AppbVcsJ689z68OxFpap8DYC1I2yKyXsKir4vMer5I1RB3nsGw8J3hv-Fzbapbrw.fIYuwlH2I CtgEyLWakayyoeUd-Z0_ejte4koBhQQphzjF-9.;Uvb:'M:zqi3Lgak' 6Y-\"bitRqS91ob3r6WAZFP'T60ZkS;IwGMhscr,wJWM q8DKn20a7AYAS5hzTH5wJYcAy,gdG\"c:cVQO1pZ,\n",
            "U\"V9h'QwX8HAa72\"N9cF85Vp_0kKYwTE\"t8IyHganTsp5bWshTQUb58q56dac0D6Qf80frgMx55F8fgK.80xW_SikgLrM;wf6SZOI2VKbR69zgFI\n",
            "Qmmu6hM6OyBKevBB8lPK-\n",
            ":'r2f6C815m5\"BBSH GWplaeqG5ywfXF'Ni5hIgOsIxN'4K56QdnFXV.SUpyry5benA6FYAsFg_9XGDg7QsvdI-FVuFaCPOENsvahTkaycaBm2I;KCQ-C\n",
            "step 100: train loss 2.4072, val loss 2.4269\n",
            "\n",
            "pow herit iobio os s ing thougng pocky t.\n",
            "d_nngod ct t dor:\n",
            "Vo2erarrrerer o - ans ye t usulstuse izecauche. tokng_as lercanson. aryovenyeyony raklis sothPe wa. thibrng_.\n",
            "Smiver relyoeralaty moth e -----\n",
            "Thave e:\n",
            "Ce woins t t'ns t4, y d an, d'ts, t; I outs d. g__j_lin_o_judeldse eermed.\n",
            "I fewss.\n",
            "My, ben y, rofindays reome s, - s_ays cul, - thitndee, t t, thatear wley inancisensorsacorstheinthat, ilinaungrud. d thaser:\n",
            "I's69ay, crrthabuline worit gr latyoul w, t yat acaisus s_gonghat, vecalex ouic\n",
            "step 200: train loss 2.3502, val loss 2.3688\n",
            "\n",
            "adans'm..\n",
            "j__wh:\n",
            "I Yourgolescthayof t t tuse Weme 25us those opom n ere nenonefure sto resu_suneref t:\n",
            "rredo veiveyoruthe ath llecheastmsid gistrme pranty, weityor alencoce onso beres onenor, tise. tendnde n oly.\n",
            "jtove aeme songeruthexel ry the thoshe cheyoesist t cha, aue An I Gf alfulloresefeinlersithes iad, tio it the t verof cr thed amatepanorlwhaiso Fe e l hathelds l ar erder isis ingeperreref sere anthe t anepicritre ose, g d al am Aron-- ptiomurenon'ty ied d ters er e crithege heisertive \n",
            "step 300: train loss 2.2075, val loss 2.2371\n",
            "\n",
            "tharer af octy hate\n",
            "se_sal_narso:\n",
            "---\n",
            "teaut:\n",
            "A at yerse.\n",
            "prer, Nousst, Juice ous, port'sefic iddm itary in alasthe orisis ly benstixay is ant eaboust, Co. Issty hap wrelero's ayiomestot or t RA n's bimpe be mpraurcansedAnd.\n",
            "j_ncovepa_rni_brsajes_ghavel:\n",
            "Wh:\n",
            "I - - y'mactee - d t ay ffrs arthe, priso t ithethithande -6an al ple - y I'rve ife tndges a fag e ton i. - omisayok I Gerthy as lllout'ticit itis -\n",
            "jond Ibuthaineyatird Thb_gerts: an bothian't wompoug I ikinknglof sins if buare t ars hive s \n",
            "step 400: train loss 1.9630, val loss 2.0079\n",
            "\n",
            "ko eve on --\n",
            "j__jon_seple:\n",
            "Gelll, chon't partes, bring buce th wo save ing\n",
            "eshyphen_ytere:\n",
            "Rit act\n",
            "machnd wigat thect siontizg outy pAbonverterrnd lion thats veilens iponge actrroul abl dendentugimiont aneal adnd ins.\n",
            "Yourller Oroby, haculend denvere coff t, ioume y, han're Bury y'son mpa en'se -catiselioues ay forincte I ay.\n",
            "de if tis, id CourWhergus d too w gie son ory a d wet vext --- se cttemerrkse lend there t se Ho tio br iaderngome inodenganely ffariresckncical tsifolers istlatht tund de \n",
            "step 500: train loss 1.7580, val loss 1.8174\n",
            "\n",
            "ere exeanced ive Could askeral.\n",
            "Thedud's pase that paplifsisicted rule.\n",
            "j__beyereh_er:\n",
            "Well, that's --\n",
            "j__breleneia_jr:\n",
            "Willl, yeven. Honss, to not's yoou recknonacys it ars I think Court.\n",
            "do_b_thereyerr:\n",
            "Well tho Ar. Gorrom yould -- resthen ould or quticeat thate o dois ear sur lay. The que Nason wan wot 198quetion to ate to abligaon, Court eskitmesat itd a wo that. Me Bresst a under ot soimayon, you to be stut by te ofall Hor of ar ito houre ave the -n wey and thid a this wolle to Buthe I. not\n",
            "step 600: train loss 1.6118, val loss 1.6780\n",
            "\n",
            "I which that do caid. Mrinice ist onlisile lank now othat's maknowsw not conraings to\n",
            "jad__samayar_all:\n",
            "Well, but a would bandn olky entend the apport the is of condule some ay, reabong say not reatre.\n",
            "But so doight -- thechens is for by the sperity its for streske, the on the Pronitione, I that youldn't was you loookeweveduldaly was sutate shve bee the lings scourcan law; keowall and the's to realked if tob7 he cart -- son's a esuing scay haves saylle imnericing to dafblicie jem frenmiaticnanab\n",
            "step 700: train loss 1.4949, val loss 1.5738\n",
            "\n",
            "j__gorguner:\n",
            "Your Honor Firce.\n",
            "And, the courtse. Knooder On for sule, you and all because are thas offor abecks your excudection of olding explecy the ortemenss contry.\n",
            "eighan_g_burisch:\n",
            "Burgh questive court Whold abitut a asain recter.\n",
            "They dort so try releath his relabson our stay trucese to achiry at pun the said courts exctive on to complarin. So, Luesausid 188Z, May Lareminauon \"a Congress dissemencly hef leall was go stamel too chain op the are eppret to he quirewar law, the Dendsttials sp\n",
            "step 800: train loss 1.4047, val loss 1.4899\n",
            "\n",
            "justs_m_knatow_delder:\n",
            "And itretect if Louting, Ind these -- a cessude.\n",
            "Ind I'd wantere a caud ditce and a pridict.\n",
            "It sa iss not thing the mony that maget. But the Court wass side dischire would pruted the statutid defing cress reviewised under you it may lim conomplore, that all a fedroce thy it had and their power 4 resan, we've know that wouldn't actory thm to coundut so how Onece Sectire.\n",
            "If then Court Nest. Code didn't have is dop strougkly sout have and a cove they athe vory how thingstre\n",
            "step 900: train loss 1.3404, val loss 1.4380\n",
            "\n",
            "Ther suld the becond this let-blikewardmentledy are wail.\n",
            "micalm.\n",
            "And they statute say dijects, if a use happly then you're a justo be they cinterses the federal hir can agriment -- that a of the motiss isciple, obrorit. Your has discriminalK gover problici_fred pagages\" oveer rulies mand we this forst that tose a disting proess making that the Thistworkem.\n",
            "Now, and I't know ways 11, can't you have said -- these appluge in fedorst stritutiong agress of rese --\n",
            "n__samuel:\n",
            "Justice Komayough.\n",
            "H_zon\n",
            "step 1000: train loss 1.2847, val loss 1.3860\n",
            "\n",
            "__j_brets_jr:\n",
            "F.\n",
            "We're bor --\n",
            "that's disopher_partiely:\n",
            "-- it will, not a 100-Arecondmerity or 19itrors of a government other copition ina in terst and for the straspopetions fow on brate to asswe refer in schem, I've marguines to been-der -- as the mal beloory luhth, what the seucciminal didncted Puedoveral claw has rearly in excremine. But What for two Court General Art, we don't troshe alil term tolace or persion on convidew be in Congress. Is wan't -- the peacim about question be peoplied th\n",
            "step 1100: train loss 1.2395, val loss 1.3534\n",
            "\n",
            "don't expense in the col simplyes, roar the injury that do will andrator, number the eventy point to lor a conseluta: of the --\n",
            "kwin infly in undgence pretaliffic that.\n",
            "so Justiona SaFIJA what maybis, we go laodatuants applicate laws object in Guckeral, then, hibs just pagroh with are outeddent reason the sticional originarsts rule. The trafeairly's may taxtes in strain that the -- there belown sufficient ministing law. I regulate, the -- that the here rule that's crearinon point in examplimity \n",
            "step 1200: train loss 1.2038, val loss 1.3130\n",
            "\n",
            "Alen't be relief the -- that Petitioner's Lacks reservated question\n",
            "Well, then appeasing of statutes here.\n",
            "They work talking and as any get to be, you comes act, for the Ry-andallrg.\n",
            "j__sonia_sotomayor:\n",
            "That's such --\n",
            "matthew_huston:\n",
            "Well, you destimis going to be Ninvolve Artigal saidittion ownelly had caremples, Mr. Taill, many be not morgerably a scall that you have justing a the factives as point position premplice overforeises. For 19 Testilly a greement issue.\n",
            "This chouldn't have a sury, t\n",
            "step 1300: train loss 1.1694, val loss 1.2929\n",
            "\n",
            "noel_r_m_unhoman_kush:\n",
            "Shank -- --\n",
            ", somaybethwinge and -- we didn --\n",
            "j__sonia_sotomayorV:\n",
            "-- my.\n",
            "josep__son_roberts:\n",
            "No.\n",
            "Thank you tring you, be car could be didn't have depal to de5\n",
            "pheril_w_lles:\n",
            "You cert, as I --\n",
            "j__sonia_sotomayor:\n",
            "No, right, we say lookle of the question, then if the cout of thill be nowen polices it.\n",
            "joseph_j_reyer:\n",
            "Thank you -- I have way to get to unlinsand was to be offider that utbody with Harnere.\n",
            "I that sairic.\n",
            "We have to as up the tifes in did without save do ununi\n",
            "step 1400: train loss 1.1456, val loss 1.2690\n",
            "\n",
            "It'd, sopulated change you applicately principlive thospers's because policy brief, approviote and presumpts, everying if is based for the fist a filed underportable that ablise that triembishously that is nempallegal\n",
            "j__stephen_g_bler:\n",
            "I think Justice Kagan.\n",
            "fre then, I -- I'do unde think, as your what is -- hosen, if even itifine the repens of then the lipice our boad itsent it is consel manswers of of their proudent status would\n",
            "j__john_g_roberts_jr:\n",
            "Well infact.\n",
            "And Congress.\n",
            "guess a -- a hi\n",
            "step 1500: train loss 1.1265, val loss 1.2539\n",
            "\n",
            "derit_b_schrasuind:\n",
            "I think what that will all to the princibility more all to individation it withi is 22 possibility issues and and invested, the general Conventraceptive capach language purpred of the in textuals challenge be to look, they dright in that the will, which never for port or minh thre.\n",
            "Act with mean, a that was argued stratrue, there's an, and -- praventing as something like but liade specifit mos.\n",
            "j__neil_gorsuch:\n",
            "Huch of the partint with that we's do don't with proass not make \n",
            "step 1600: train loss 1.1039, val loss 1.2400\n",
            "\n",
            "Inthat's the official review discriminate was as alsen really broth weing a proper of the regulation to removal way, it back a later sition proceedings that not unegolators in pitace. The owner offeact they wanteven draw-memalutel imposition interpreding.\n",
            "It you're asking than a letterment.\n",
            "That's a marks rule cirto or at an yeaE says would be regulater. The separagration sex.\n",
            "And so it worsent pening Act like question One tests, under they given broug is. Tecreatierly case, they could doecades,\n",
            "step 1700: train loss 1.0914, val loss 1.2219\n",
            "\n",
            "ed did efortaive to act within everybody fase, incirce modeted exemptions\n",
            "crimatther_p_p_her:\n",
            "Your Hell6O's mut -- and the -- this and our hele draw expertion opporte two be and, oh, which there's not no donarsh with tradiction or tif same strougge because, how Phresent's disagrapaged who not particular language and I-- all -- includes --\n",
            "j__sonia_sotomayor:day If I stide active moven to question; is case me schall me said facts the same law substant to set toll second-regible. If -- it's someth\n",
            "step 1800: train loss 1.0819, val loss 1.2232\n",
            "\n",
            "I'm agree 1 for anything Florida Or Oklahoma, the motion Justice I hught We different.\n",
            "And agreement knows. So, I think, and we use this system that is says, in this is that a feith pledge Way, in my demore little need for this about the problem appech-fulction.\n",
            "And I think, it was as Bying Aside. If reconsizes about person.\n",
            "As right, I'd show -- I --\n",
            "j__ruth_bader_ginsburg:\n",
            "That's an agree that three indives and there's grett the but understain.\n",
            "So not does we have suit the state tilling to ban\n",
            "step 1900: train loss 1.0598, val loss 1.2078\n",
            "\n",
            "Onder itself, in the funstation that changes would court with that, just how Two decly ack this is an acxult bemately that decision Mentgor. Riale So I would interate says qustions a cort says just abset to more where than corns shave what he question within it\n",
            "aminguel_eding:\n",
            "The judgment --\n",
            "j__clarence_thomas:\n",
            "Take you crodosly that's exply question arbitrations for he -- is not anything that, they have that, there reving actually mexself-causes to be allonged to line Buven and any and Nobnoti\n",
            "step 2000: train loss 1.0481, val loss 1.1933\n",
            "\n",
            "oraminatration of his haven narrow deliging allowing it sence and then he city, makes -- to -- perfectly in the regulations that this -- the refer the President's profits answers the -- when the -- I think a case.\n",
            "j__brett_m_kavanaugh:\n",
            "I do know cited that jy -- even it.\n",
            "eric_j_feillia:\n",
            "It decides think that was a substance state. Congress, saying sere filed -- but o different, well, right courts restribute. I think -- we mentive would not on till source. All this law as my timply employment or \n",
            "step 2100: train loss 1.0390, val loss 1.1839\n",
            "\n",
            "Congress the movern tax againstramence two, generaless three\n",
            "paul_w_hugheler:\n",
            "We didn't could asse costs --\n",
            "j__stephen_g_breyen:\n",
            "All the FTOA, else, it would be not place that were still require --\n",
            "j__sonia_sotomayor:\n",
            "So whatever you degolate clause put that\n",
            "lasela_s_ky:\n",
            "Do your 1212, when ho know off, your argument letimit to say and had, as yeah.\n",
            "And you think humber to admithed contract provides that case.\n",
            "What I questilly in this Conventon will, accludation the -Hurthers like somethooms. And\n",
            "step 2200: train loss 1.0268, val loss 1.1801\n",
            "\n",
            "And'll right. So Mr. Kagan, I think you -- you would be got amen_let gil you ask on a burpres\n",
            "jonathan_ellis:\n",
            "This Court, brief Justice, Justice, and in many at out ther keepars, if I vittle meants, who -- serious put the rise --\n",
            "frederick_lius:\n",
            "Yes, You know, let's conceation that amendment sounds this savile, because the think that line it point hument respe troifts certained Phetice.\n",
            "I and I'm try, there's not not-on-constribution cases that the can be right, but I was -- read make proforts t\n",
            "step 2300: train loss 1.0229, val loss 1.1750\n",
            "\n",
            "And that's question that I was trial part amount.\n",
            "Justice Whoever, it want to member the Justicer was where the Patent point's And I'm agree in Somalic provision insure there is a problem. And if they were there trying the speaking arenal law, but I let's take about it, at like they authorical years they already orm and the requirements that scome federal tew mealved for in ane more for decision. Are I this are triving out under andought.\n",
            "Those here 9 they are overy remainding in this sprior pro\n",
            "step 2400: train loss 1.0146, val loss 1.1678\n",
            "\n",
            "Thank so that is reasonable standard.\n",
            "And that sentence the best that least on is very fees, but that he's called in common in the foutural question day excert agaic for his have spended forger to bpositive proble\n",
            "philip_j_weiser:\n",
            "Your Honor. The -- one has is a familitiar job, not the regishle in -- even in an even two doesn't not reason would be think that you're not buyrichted on aborking wrong. Than women's not libe.\n",
            "That's point some protection for the law crusiden.\n",
            "And that's incentional.\n",
            "\n",
            "step 2500: train loss 1.0047, val loss 1.1651\n",
            "\n",
            "adam_g_unikoss:\n",
            "Right to trise -- because I think -- that Kansas something everyonging evided --\n",
            "j__elena_kagan:\n",
            "I take your or verse agge, all or what we are justicial relief convicting the Court hespens. Nobody.\n",
            "joseph_r_palmore: I'm sorry. A parte manum for fiduciary.\n",
            "j__sonia_sotomayor:\n",
            "Ef you, conce -- Justice's Gins, and ever than the first fact.\n",
            "On the time is sub:\n",
            "We eaken is a partical -- the final property standards'; is the stol of the status of it entity. So Just questions all his cl\n",
            "step 2600: train loss 0.9948, val loss 1.1586\n",
            "\n",
            "ed_j_feiser:\n",
            "-- I would require that doesn't become to the -- the -- the 16 people who can't remains are are not reconstated the bettegened. In 2166 appropriate on this.\n",
            "And if you go Montgomery is meant, you know, the Court would the ch, Chief would be objectively canfulles because he's petition.\n",
            "There's not requirement.\n",
            "An that the scheme off that child beyond you have to give into annothic and the befits at the Enablim Cleent to correct the text and deIt sopple form what either anyone to sepa\n",
            "step 2700: train loss 0.9904, val loss 1.1566\n",
            "\n",
            "j__stephen_g_breyer:\n",
            "That's the action is violations from the judgment, and if you can address go the drewin or should be suggest, it's embatermandum transing for here clear to apply. We to the paral person with your -- the owner it are surveating for exepuries the decision. So you frame talysized would you say lead have difference comained imposed regulation.\n",
            "I to show you what you has asn't Cased cases liked to stand.\n",
            "To don't conduct inclosses to I have the -- the term other there is no resul\n",
            "step 2800: train loss 0.9811, val loss 1.1503\n",
            "\n",
            "This would be awackgred finding. The fact is -- issen for those Taggliht Justice Paterherf's harms to the reason are primarily done affirmiliary we're not make sure this that's enough sometimewing --\n",
            "j__brett_m_kavanaugh:\n",
            "-- and really non-\n",
            "richard_p_deari:\n",
            "Well, I'm not going to -- ryong -- your point there basis.\n",
            "I mean, we cases agree with that discrimination because the second in the hree for hotweight, I adopt my client the need Nating to state add --\n",
            "j__brett_m_kavanal:\n",
            "-- orrder on a legi\n",
            "step 2900: train loss 0.9781, val loss 1.1457\n",
            "\n",
            "morgan_t_l_raff:\n",
            "Let's --\n",
            "j__sonia_sotomayor:\n",
            "erica_l_ross:\n",
            "-- Justice Ginsburg. Hall is.\n",
            "j__sonia_john_ginsburg:\n",
            "So you -- General, particularly was -- was a fest permited under Petitioners.\n",
            "And there's no election uncoversy instured Your Honor did not grad that the government that shouldn't count upon Ariery, as to Justice Sotomayor.\n",
            "I justice about the way resolutions, that the deference to do is under the nettle 2C, and to the bottle bigg -- because we mpotively fav the borders of extractly \n",
            "step 3000: train loss 0.9717, val loss 1.1402\n",
            "\n",
            "morgan_l_ratner:\n",
            "I mean, the question 32ed to me avoid for Court do.\n",
            "And Congress have recognized the Title Wallin What is doing\n",
            "enica_l_ross:\n",
            "Yes, security, however, and --\n",
            "j__stephen_g_breyer:\n",
            "You're -- the --\n",
            "shay_dvoretzky:\n",
            "It citcrate.\n",
            "j__elena_kagan:\n",
            "-- it clearly srected the Fourth Amendment in the federal purpose histomy and people on the Judge --\n",
            "philip_j_joshey_bursh:\n",
            "Some because they beyond the -- I and the time is up the director punch about in Lanch He arberry avail; that was to it\n",
            "step 3100: train loss 0.9646, val loss 1.1302\n",
            "\n",
            "samuel_bonderotoff:\n",
            "But trust, Your Honor's -- like -- years the remedy is generally risked hards adfer the statute by their right assitution or fiduciary to the mister option that is a question, and they courts ar indicated the need in evalingforment issued and resolve them it to be assumer before of she orgary that simes resolve in subjetct of yansgs in the lecenser brief.\n",
            "But if I we didn't think that there's a more in cases is that individually and kind of the day or hing in equity a4 and, a\n",
            "step 3200: train loss 0.9629, val loss 1.1380\n",
            "\n",
            "Ms. Here. Do, does, not they nature very 6,0000 permiss; in Prehatens tell provides to say them from the Amendment can't be contraced by them.\n",
            "It does that at a violation valid convicate. Any answer that we have a parama vote, so that you doing in going terrorizing for -- a uncrelated the second supprisary may this acsting proceeding that Somebody Groodbach who are acknowledged for anything constructive sike coverage element decloss that would -- and you alserse whether there's an hight actin qu\n",
            "step 3300: train loss 0.9542, val loss 1.1332\n",
            "\n",
            "morsuan_elatiner:\n",
            "Yes.\n",
            "j__stephen_g_breyer:\n",
            "The presidents and have now that the conduct match are that that case Ritzen -- and will their real. Montana. So you have to have a call hosnez me and --\n",
            "j__john_g_roberts_jr:\n",
            "I -- I -- I don't know -- I was looking at sciep, it's a safeloniance of the violation, I really with and a rulist portion when is, certiganting there, enforce is entircumble. If they're going to to me turriented to like which you ago allowed the another question is whether the o\n",
            "step 3400: train loss 0.9508, val loss 1.1248\n",
            "\n",
            "On this weaks. The state -- the one time that and not those are centings.\n",
            "The defendant, in the coffense and the final end rule redhard. First, well, we think that the state werean, in 1961, we judge -were the big the language number on controls. And if you are Porresidential injioncy, with a hum. So, we're -- the fuse of affirmative allegenties and they're truling in been 15, we think they annotated -- they are altern's meant ono exactly quest that may.\n",
            "j__john_g_roberts_jr:\n",
            "What's your argumen\n",
            "step 3500: train loss 0.9433, val loss 1.1197\n",
            "\n",
            "join_l_roberts_jr:\n",
            "So do you pichase there are a violation already in sentencing decision for the -- the faiting case not begin for any error being sentencing for their way, just to have a small of jurisdiciations, always to effect, because of contraceptivele decisions.\n",
            "We gave more that seem, you don't see to me at this different by characteristics consider asks and some of seconding schargeness in that baservel to the reconstent to the district charged, she whatechniess ever. Herefore, since t\n",
            "step 3600: train loss 0.9403, val loss 1.1255\n",
            "\n",
            "malcolm_l_stehoman:\n",
            "The final disgorgement because it was you decid the state because of all, they're specially in that for worth out --\n",
            "j__neil_gorsuch:\n",
            "Why would we st -- they're not supposed under the same --\n",
            "theodore_b_olson:\n",
            "DI think about the two -- answers thing, Your Honor, which may well both that Tottombi doectribes' positions, well, we do get to first that somebody of these typies choice. But, we filed 10 moymbers teachings, because we don't exercise if the district main occurred legi\n",
            "step 3700: train loss 0.9337, val loss 1.1221\n",
            "\n",
            "-- how have a minimis and so it.\n",
            "So --\n",
            "jonathan_d_bond:\n",
            "-- I -- I'm not -- I'd like to be something that.\n",
            "So I'm just sure -- always -- hokes -- is about a result, but it sound out that it is not securing the court to acknowledge that which --\n",
            "j__elena_kagan:\n",
            "So --\n",
            "j__john_g_roberts_jr:\n",
            "-- was at factually, schoolard, which sounds --\n",
            "anthony_a_yance:\n",
            "You --\n",
            "j__neil_gorsuch:\n",
            "-- that's own.\n",
            "j__stephen_g_breyer:\n",
            "-- If somay you across -- you raising what Congress out could say much that your amicus\n",
            "step 3800: train loss 0.9309, val loss 1.1187\n",
            "\n",
            "derenk_l_shaffer:\n",
            "And -- and it doesn't said it as an everything els, Your Honor\n",
            "Absolutely, but I don't think it's a part of your to pay you question. When we thought is something probably bewhalf it this is wrong in then to look baad, has meritured a strate distinctly that changes here too do the reason for a -- or isn't any reason foreign that Predication if it was to mean in Foti. I'm not awarding.\n",
            "eric_j_feigin:\n",
            "So you mind this case, years because there's no encourage inappolication over c\n",
            "step 3900: train loss 0.9276, val loss 1.1160\n",
            "\n",
            "miguel_a_estrada:\n",
            "-- did not anything annot the Imagin, isn't supply but when he used one -- a stage everybody and when it was more and -- and apointed to the least to --\n",
            "j__neil_gorsuch:\n",
            "Okay. And what -- which y anso are liketh anyone providing, plaintiffs like the -- the room from some that Monta -- the women's noticed at a period poips of reemoval. And this Court has less in cases, I don't think --\n",
            "joseph_r_palmore:\n",
            "-Would about have it said one.\n",
            "j__sonia_sotomayor:\n",
            "I -- I mean, merge it's n\n",
            "step 4000: train loss 0.9243, val loss 1.1172\n",
            "\n",
            "paul_d_clement:\n",
            "And -- and would I think that she --\n",
            "j__stephen_g_breyer:\n",
            "Couldn't ullege --\n",
            "noel_j_francisco:\n",
            "-- standard -- has kinds --\n",
            "j__ruth_bader_ginsburg:\n",
            "-- if you haven't almost turned, underther the trial courts of process by salitrated.\n",
            "So I'ct recent, yet age to do Cine IIn I can't just not a judge what jury\n",
            "jeffrey_b_wall:\n",
            "I -- for being discrimination could say: Justice Aland I thought that I think, you're not fane loast tolling is going to back and shot under the rangement and th\n",
            "step 4100: train loss 0.9184, val loss 1.1111\n",
            "\n",
            "Studants, where the buyer property traffic is now that ties that are not whether President, is -- is of -- diah-discriminational law claims that -- from the Court should in -- in this and tat is recently of a complaint, and as this was not a single idention And would assume convict the one whomen at issue no good a release and governs and reason to the Form, which I have served Resor and Hosalters.\n",
            "And Security 18a resports to Article Would are subject and, whise the ideason inadmissibility pres\n",
            "step 4200: train loss 0.9163, val loss 1.1032\n",
            "\n",
            "I think when it may picked that race as true to defend callel had generally federal -- surely, I would think it nearly have talking about more at issues from the Congress. Now, so it certains did epirical that that's what the theory about me that's a more cause poice took, it ormedian 19055a, and whether the concept historical taxpays, before the government on that, is exception that usually be past by either or the damage.\n",
            "j__stephen_g_breyer:\n",
            "I understand, section 12 is that the statement exce\n",
            "step 4300: train loss 0.9084, val loss 1.1004\n",
            "\n",
            "The judgments upoted the -- the nine-Madic Anjemendating continuition offenses. The -- the Hock judge can scerio be --\n",
            "j__elena_kagan:\n",
            "But you can -- Gil_gorsuch:\n",
            "Right.\n",
            "j__elena_kagan:\n",
            "Mr. Sherry calls -- can be facted.\n",
            "But, for example, if -- if they can't fortuute the distinction.\n",
            "And there'sn't a readiod in the instatuto cases in a different abstract, but the BM- D Cokes pipes, but I am trust explained question, with the normer just going to delie whethele cannot going to apply forward.\n",
            "Othe\n",
            "step 4400: train loss 0.9048, val loss 1.1008\n",
            "\n",
            "Congress has to prose question because I don't think there are clearly approaching in 1975, a collection -- does avoked a -- a general defendant judge filed the mandating the provision whether the barjecause is properly religious --\n",
            "j__neil_gorsuch:\n",
            "-- would not be saying that.\n",
            "j__neil_gorsuch:\n",
            "The -- the -- the subpoenas meant by the scope or, the Clevelanation is picked of its beneficial to telness 2 of the voter of this titled your issue.\n",
            "I guess in this case off a Joined II as a voting thing\n",
            "step 4500: train loss 0.9047, val loss 1.1058\n",
            "\n",
            "frederick_limita:\n",
            "Justice Ginsburg:\n",
            "Well, I think so, I just set you need acted to pleadical regulations, you know, do it apply to the court, that's not -- the -- the priod of bor sort of light distriuct to gett Luckerike a stronge Tabur direction or. And, therefore, most view, I understand that a doing to not be, old let's just go back and at there all of things to be kind of the statute on -- omeon that potentially benefit, on memoreoved saying it foreign remark and it is every sort of people \n",
            "step 4600: train loss 0.8981, val loss 1.1003\n",
            "\n",
            "julie_rikelman:\n",
            "I guess the same director than -- I'm sorry, the difference in public dispute is wrong.\n",
            "It means of your best acception is the Acts, I disagree with that my all officials out postrums, for example, -- no, ultimately make set ome lars whole buck knowledge beigge make the cases says, it looks to the conviction of the here is that any knew of that case, likedman mindamus and that feels may be worth an exactly the fish is faither fairs to plose on page. We think your argument change \n",
            "step 4700: train loss 0.8985, val loss 1.0996\n",
            "\n",
            "j__brett_m_kavanaugh:\n",
            "May I -- I'd like to begin on what standard.\n",
            "frey_m_citron:\n",
            "That's whether the problem with resent with the preempted.\n",
            "j__brett_m_kavanaugh:\n",
            "Well, then the respond to something out by the Bottlem it does mention by a party has xceeding rout or three like. Diddd so you that can't really beyond punil --\n",
            "j__sonia_sotomayor:\n",
            "-- confilsed factors have agreed, for the only law lony unnusures stair could contract, but-for example, harm to extension no only issue and I want to make\n",
            "step 4800: train loss 0.8942, val loss 1.1009\n",
            "\n",
            "420s for Jones 14 states. Now, can then, in other wordinary, making of it proots, we're talking about any found of this way. The Civil Convention is saying that Congress is revested on Corport to have covered for what the pudition can review fund where the statute of my friends have to phons historers; mucholerture.\n",
            "The court seems to do with -- the meaning that even if the government has the vacalian is that proques consider fiduciary duty says a selft saying what the wroten tests adopted an un\n",
            "step 4900: train loss 0.8891, val loss 1.0974\n",
            "\n",
            "So --\n",
            "j__sonia_sotomayor:\n",
            "The FCC did not address the poer has to go about securities criminal is to might say that it depends on history of the role is bed in that case there case, but is present oursider review as a possible admissal, not and, discussive, sort of these Tuchory, doesn't this Court's question about what there are the New W-part wills frill you will lose up about that Section 1166 that Mr. T. Oltmer, in the Court, those are reversed that kind of -- of the clievence you doll you t\n",
            "step 4999: train loss 0.8893, val loss 1.0958\n",
            "\n",
            "kelin:\n",
            "The director comes --\n",
            "j__ruth_bader_ginsburg:\n",
            "Exactly there, assuming your --\n",
            "michael_b_kimberly:\n",
            "-- to inpress -- pur brief. But in Article IER Nor cases.\n",
            "j__ruth_bader_ginsburg:\n",
            "But you suggest that the transfers who are not reason to perman of all\n",
            "versheot.\n",
            "adam_g_unikowsky:\n",
            "General --\n",
            "j__ruth_bader_ginsburg:\n",
            "Well, Congress -- to negligents.\n",
            "erica_l_ross:\n",
            "So Your Honor really says, look, I think, I think you would, and with before we ould post to remand it as given shows to amend to in\n",
            "\n",
            "Freder particular ssecurispition settled privileges to this.\n",
            "Due 3,000 years there are reasons.\n",
            "And is a defined, this Petitioners can be pull thems. It says, if they're asking that this morning DACA to due people of a murder, it'll presume the independent sure.\n",
            "If there's a -- if the definest expanst will blieve discussine the oppowenes I am iniseally asking Flor herns, to the Court take because it's a sex -- and that part of the -- the traffs suit study normally does. That wouldn't be on one o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open('/content/drive/MyDrive/gpt/more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "id": "LjRr5z9_nqu7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10be055f-623a-49cd-e04b-420124f8d439"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10001"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}