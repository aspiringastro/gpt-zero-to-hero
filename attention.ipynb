{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://jaykmody.com/blog/attention-intuition/\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}\n",
    "\\end{equation}\n",
    "\n",
    "In this formula, $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ are the query, key, and value matrices, respectively, and $d_k$ is the dimensionality of the key vectors. The softmax function is applied to the dot product of the query and key matrices, normalized by $\\sqrt{d_k}$, and the resulting weights are used to weight the values in the value matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
