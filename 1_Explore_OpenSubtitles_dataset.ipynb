{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "280976db-27e8-4846-b98a-905645599ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d3a66ec1-b7b5-481a-98ec-671d60fc1b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the first million lines of the opensubtitles 2018 dataset\n",
    "# Reference: Citation J. Tiedemann, 2012, Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)\n",
    "\n",
    "with open('data/tiny_opensubtitles.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Remove all non-alphanumeric characters\n",
    "import re\n",
    "text = re.sub('[^A-Za-z0-9.,!\"\\':;_ ]+', '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "05c7daea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 10002825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Presented by IM PicturesProduced by Shin CineIn association with MVP Venture Capital and Cinema ServiceJeon Jihyun Cha TaehyunMy Sassy GirlExactly two years ago today, she and I buried a time capsule here.We promised to meet here two years later, but she hasn't come yet.I'm going to wait.Here we go.Please, don't move.One, two...Wait a minute.HelloOh, auntie.Sorry, I'm on my way.I'm really sorry.Yes, I'm coming.I'm having my photo taken.Bye.Are you readyHere we go.One, two...My parents wanted a daughter, so they raised me like one.So I thought I was a girl until I was seven.I had to go to the women's public bath, too.The older I got,I thought my penis would get smaller and disappear.But it was the opposite.First HalfHe hasn't changed at all.No, I'm a real man now.Hey, asshole.Think clerical work in the army makes you a manYou irritate me!Give me a break, asshole.My job was tougher than you could imagine.Hey!I worked near the DMZ.Who are you kiddingHold it.Anyway, welcome back home.She's\""
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Dataset length: {len(text)}')\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "823d2eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"',.0123456789:;ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz\n",
      "Length of vocab: 71\n"
     ]
    }
   ],
   "source": [
    "# all unique characters in the dataset\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(f'Length of vocab: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f6a99a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 49, 56, 56, 59, 0, 37, 62, 45, 58, 63, 50, 59, 62, 57, 49, 62, 1]\n",
      "Hello Transformer!\n"
     ]
    }
   ],
   "source": [
    "# create a mapping of characters to integers\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "#encoder: take a string and return a list of integers mapped to that string\n",
    "encode = lambda s: [stoi[ch] for ch in s] \n",
    "\n",
    "#decoder: take a list of integers and return a string\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "str = \"Hello Transformer!\"\n",
    "print(encode(str))\n",
    "print(decode(encode(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "32b04892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10002825]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire dataset and store it as a tensor in Torch\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f461f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length: 9002542, Valid data length: 1000283\n"
     ]
    }
   ],
   "source": [
    "# Train and Validation data sets\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f'Train data length: {len(train_data)}, Valid data length: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7f8e0913-ccbc-4352-82ed-8dc0acf24275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([33, 62, 49, 63, 49, 58, 64, 49, 48])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ac23b74f-8ac7-4dab-861e-1b88cd1ff31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input in tensor([33]) (P), target is 62 (r)\n",
      "when input in tensor([33, 62]) (Pr), target is 49 (e)\n",
      "when input in tensor([33, 62, 49]) (Pre), target is 63 (s)\n",
      "when input in tensor([33, 62, 49, 63]) (Pres), target is 49 (e)\n",
      "when input in tensor([33, 62, 49, 63, 49]) (Prese), target is 58 (n)\n",
      "when input in tensor([33, 62, 49, 63, 49, 58]) (Presen), target is 64 (t)\n",
      "when input in tensor([33, 62, 49, 63, 49, 58, 64]) (Present), target is 49 (e)\n",
      "when input in tensor([33, 62, 49, 63, 49, 58, 64, 49]) (Presente), target is 48 (d)\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input in {context} ({decode(context.tolist())}), target is {target} ({decode([target.tolist()])})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "489c4670-ce3e-4830-905b-d4e47ce8f782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([8, 8]), targets: torch.Size([8, 8])\n",
      "xb =  tensor([[59, 58, 67, 59, 59, 16, 18, 58],\n",
      "        [ 4,  0, 57, 69,  0, 56, 59, 62],\n",
      "        [25, 45, 58, 55,  5, 26, 64,  3],\n",
      "        [50, 56, 65, 49, 58, 64,  0, 57],\n",
      "        [11,  0, 58, 53, 51, 52, 64, 63],\n",
      "        [62,  0, 62, 45, 64, 52, 49, 62],\n",
      "        [56,  4,  0, 26,  0, 52, 59, 60],\n",
      "        [49,  4,  0, 64, 52, 53, 63,  0]])\n",
      "yb =  tensor([[58, 67, 59, 59, 16, 18, 58, 48],\n",
      "        [ 0, 57, 69,  0, 56, 59, 62, 48],\n",
      "        [45, 58, 55,  5, 26, 64,  3, 63],\n",
      "        [56, 65, 49, 58, 64,  0, 57, 49],\n",
      "        [ 0, 58, 53, 51, 52, 64, 63,  0],\n",
      "        [ 0, 62, 45, 64, 52, 49, 62,  4],\n",
      "        [ 4,  0, 26,  0, 52, 59, 60, 49],\n",
      "        [ 4,  0, 64, 52, 53, 63,  0, 67]])\n",
      "------\n",
      "when input in tensor([59]) (o), target is 58 (n)\n",
      "when input in tensor([59, 58]) (on), target is 67 (w)\n",
      "when input in tensor([59, 58, 67]) (onw), target is 59 (o)\n",
      "when input in tensor([59, 58, 67, 59]) (onwo), target is 59 (o)\n",
      "when input in tensor([59, 58, 67, 59, 59]) (onwoo), target is 16 (:)\n",
      "when input in tensor([59, 58, 67, 59, 59, 16]) (onwoo:), target is 18 (A)\n",
      "when input in tensor([59, 58, 67, 59, 59, 16, 18]) (onwoo:A), target is 58 (n)\n",
      "when input in tensor([59, 58, 67, 59, 59, 16, 18, 58]) (onwoo:An), target is 48 (d)\n",
      "when input in tensor([4]) (,), target is 0 ( )\n",
      "when input in tensor([4, 0]) (, ), target is 57 (m)\n",
      "when input in tensor([ 4,  0, 57]) (, m), target is 69 (y)\n",
      "when input in tensor([ 4,  0, 57, 69]) (, my), target is 0 ( )\n",
      "when input in tensor([ 4,  0, 57, 69,  0]) (, my ), target is 56 (l)\n",
      "when input in tensor([ 4,  0, 57, 69,  0, 56]) (, my l), target is 59 (o)\n",
      "when input in tensor([ 4,  0, 57, 69,  0, 56, 59]) (, my lo), target is 62 (r)\n",
      "when input in tensor([ 4,  0, 57, 69,  0, 56, 59, 62]) (, my lor), target is 48 (d)\n",
      "when input in tensor([25]) (H), target is 45 (a)\n",
      "when input in tensor([25, 45]) (Ha), target is 58 (n)\n",
      "when input in tensor([25, 45, 58]) (Han), target is 55 (k)\n",
      "when input in tensor([25, 45, 58, 55]) (Hank), target is 5 (.)\n",
      "when input in tensor([25, 45, 58, 55,  5]) (Hank.), target is 26 (I)\n",
      "when input in tensor([25, 45, 58, 55,  5, 26]) (Hank.I), target is 64 (t)\n",
      "when input in tensor([25, 45, 58, 55,  5, 26, 64]) (Hank.It), target is 3 (')\n",
      "when input in tensor([25, 45, 58, 55,  5, 26, 64,  3]) (Hank.It'), target is 63 (s)\n",
      "when input in tensor([50]) (f), target is 56 (l)\n",
      "when input in tensor([50, 56]) (fl), target is 65 (u)\n",
      "when input in tensor([50, 56, 65]) (flu), target is 49 (e)\n",
      "when input in tensor([50, 56, 65, 49]) (flue), target is 58 (n)\n",
      "when input in tensor([50, 56, 65, 49, 58]) (fluen), target is 64 (t)\n",
      "when input in tensor([50, 56, 65, 49, 58, 64]) (fluent), target is 0 ( )\n",
      "when input in tensor([50, 56, 65, 49, 58, 64,  0]) (fluent ), target is 57 (m)\n",
      "when input in tensor([50, 56, 65, 49, 58, 64,  0, 57]) (fluent m), target is 49 (e)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # whhat is the maximum content length for prediction?\n",
    "\n",
    "def get_batch(split):\n",
    "    #generate a random batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # create a block_size worth of random indexes within the data - blocksize length\n",
    "    ix = torch.randint(len(data) - block_size - 1, (block_size,))\n",
    "    xb = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    yb = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return xb,yb\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(f'inputs: {xb.shape}, targets: {yb.shape}')\n",
    "print('xb = ', xb)\n",
    "print('yb = ', yb)\n",
    "print('------')\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'when input in {context} ({decode(context.tolist())}), target is {target} ({decode([target.tolist()])})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "21660659-8c51-4eb3-bba5-e98c27305aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[59, 58, 67, 59, 59, 16, 18, 58],\n",
      "        [ 4,  0, 57, 69,  0, 56, 59, 62],\n",
      "        [25, 45, 58, 55,  5, 26, 64,  3],\n",
      "        [50, 56, 65, 49, 58, 64,  0, 57],\n",
      "        [11,  0, 58, 53, 51, 52, 64, 63],\n",
      "        [62,  0, 62, 45, 64, 52, 49, 62],\n",
      "        [56,  4,  0, 26,  0, 52, 59, 60],\n",
      "        [49,  4,  0, 64, 52, 53, 63,  0]])\n"
     ]
    }
   ],
   "source": [
    "# Input to the transformer\n",
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b681ba06-6ebe-4863-97de-e625a1fe8122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a Bigram LM using Torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # ifx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) #(B, T, C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens=100):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(idx)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # shape is now (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e49e39b2-b58f-4394-a0ae-b969b4c71f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 71])\n",
      "Loss = 4.904510498046875 - based on vocab_size 71, expected loss is $ln(1/vocab_size)$ -4.2626800537109375\n",
      "Generated Text:\n",
      "\n",
      " N25idzV5oLiJhyc6uR71 ,V5;f5ht8w77Wl92yQIQIgqAmvBniN7BfIT dITaHuOLQ4!947bYYuzPDGae7RhJ8skbf:HX_if;.3u\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(f'Loss = {loss} - based on vocab_size {vocab_size}, expected loss is $ln(1/vocab_size)$ {torch.log(torch.tensor(1/vocab_size))}')\n",
    "\n",
    "print('Generated Text:\\n')\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long))[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "74c09f7f-6602-4ab8-95e1-0739df2f1de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b785e2e1-7746-48ed-9212-60e2d112b716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8807783126831055\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "print(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ae97ce89-14be-49d0-923c-33e514af50be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " j:kTgqZr9gXLNej_ITvv;AAR1B jnHAhyjwNTe75aeT,.B1FTI9VYMOGFWbV,VtBe,LU P7RhrL.Qz d'bLIqEBM T;zPF,Bpp15S1j_kV rlPB9;tENCKkkO\"z\"dadRFX,znwMOe\"i2dv3OhWXH,yVsFcN6 dPWGKk rMKuyIDGFM wMCDkLVX5vL!1Ztc'RhpQwI6'fN2dvjMQvG:k4bxm6ITI3E2poPjBpUeVxBM.v0fO91G9kRhRJAp'8DnoGaC wS'hMmPwVXLAKHHN\"FZmG'JC\"F,.6uZ;UU lj8,j9YVifE1nRdVU0wMOzgEJjF6eOJtfoP3IXCnCLqhj'1zVX5.HHge1C;fOIwVtca3I9UPDmG2GlPVUzc:2mhyj4aSOhc d6QQGMDxM_gRhrQ:L1mobLkXL8iWOU:6Hcj;tcjph4Adh9'w NT1BAc MK2bc:IDsLQnVJ.V13v_n8u;i4 Tc23e\"9p\".v_:oszGdZDcaiv.v\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5be111-2faf-47e3-b5be-f6652db37d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
